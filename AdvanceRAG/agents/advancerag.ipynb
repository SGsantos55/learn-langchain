{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368a221d-53ba-40f9-a633-1e4ad3453eaf",
   "metadata": {},
   "source": [
    "  ## *RAG WITH MULTI DATA SOURCE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b36a364-bedf-4d17-981a-92c3d6bae996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #to access the tools like wikipedia arxiv we will be creating wrapper for individual tools in \n",
    "# top of it and later we will combine it with user and llm to create chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f77ffc1-6828-4274-af42-48e5aa6ae2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "#this will be responsible for running the wikipediaapi wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b8f805-a382-4a5f-8344-c314a38805ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "# this wrapper will hit wikipedia api and will provide the response according to my query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb269e74-2676-4c02-a1e4-f102cb6b465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=200)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d45808-e5bc-460f-991b-c0d884fdccc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86f16ea0-c060-4e04-97ea-6042b0eaa29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santosh/ml_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-22 21:05:57.157482: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-22 21:05:57.663768: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-22 21:06:00.520351: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/santosh/ml_env/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5dde3f5-d602-4f62-9896-23af6a0acfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8642/1474760240.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b046ce9d-4450-4caa-92aa-724653424783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7264c039f8c0>, search_kwargs={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://pmc.ncbi.nlm.nih.gov/articles/PMC7983091/\")\n",
    "docs=loader.load()\n",
    "##create chunks of docs \n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "vectordb=FAISS.from_documents(documents,embeddings)\n",
    "retriever=vectordb.as_retriever()##this one is default retriever tool it's a faiss retriever not a langchian tool \n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b621ba66-5c63-40f0-a623-a9e2390bd91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ##we will create our own retriever tool \n",
    "\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"pmc_retriever\",\n",
    "    description=\"Search for info about machine learning algorithms and their real world applications and research directions.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ec296f0-50d5-46a7-94e4-6b54ddc363a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pmc_retriever'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0478373e-c4d9-46f2-8001-855e0b77707e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##arxiv tool \n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "\n",
    "arxiv_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6a5f0c6-344f-42e9-8aa3-db4beae2c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[retriever_tool,arxiv,wiki]#[retriever_tool,wiki,arxiv] only retriever tool for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "05ff4bf2-19fb-4591-8572-38ec6122c4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='pmc_retriever', description='Search for info about machine learning algorithms and their real world applications and research directions.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=<function create_retriever_tool.<locals>.func at 0x72639911a8e0>, coroutine=<function create_retriever_tool.<locals>.afunc at 0x72639911a5c0>),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/home/santosh/ml_env/lib/python3.12/site-packages/wikipedia/__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200))]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb51d606-5d0f-4e3c-b203-b806a7feb68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ['GROQ_API_KEY']=os.getenv(\"GROQ_API_KEY\")\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "744d439d-fbed-4ce0-9e72-a58ffbb87900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #we can pull prompt template from hub also \n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"You are a helpful AI assistant. Use tools when necessary.\"),\n",
    "#     MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "#     (\"human\", \"{input}\"),\n",
    "#     MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "48e03475-471f-4420-91b5-048fa71d2dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##AGENTS\n",
    "from langchain.agents import create_agent\n",
    "agent=create_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    system_prompt = \"\"\"\n",
    "You are a helpful AI assistant. \n",
    "\n",
    "Rules:\n",
    " \n",
    "1. For reasoning, document-based, or multi-step questions, use tools (retriever, wiki, arxiv) as needed.  \n",
    "2. Be concise, accurate, and structured in your answers. \n",
    "3. Never call more than one tool per question unless absolutely necessary. \n",
    "4. Always provide references from documents when requested.\n",
    "\"\"\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "583edd35-149f-4df4-86c9-c830c9d8793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\n",
    "    'messages': [{\n",
    "        'role': 'user',\n",
    "        'content': \"What are the recent advances in representation learning using self-supervised methods, and how do they compare with supervised approaches?\"\n",
    "    }]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2e4ce53-6688-47cc-8f84-1b0e92becd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on human-provided labels. This approach has gained popularity in recent years due to its ability to learn representations that are useful for a wide range of tasks, without the need for large amounts of labeled data.\\n\\nOne of the key advantages of SSL is that it can be used to learn representations that are invariant to certain transformations, such as rotations or translations. This is because the model is trained to predict the output of a transformation, rather than the transformation itself. For example, a model trained on images of objects with different rotations would learn to represent the objects in a way that is invariant to rotation.\\n\\nSSL can be used in a variety of applications, including computer vision, natural language processing, and speech recognition. It has been shown to be particularly effective in tasks where the data is high-dimensional and the number of labeled examples is limited.\\n\\nThere are several types of SSL, including:\\n\\n1. Autoencoders: These are neural networks that learn to compress and reconstruct data. They can be used for SSL by training the model to reconstruct the input data, rather than predicting a label.\\n2. Contrastive learning: This involves training a model to distinguish between positive and negative examples. For example, a model might be trained to distinguish between two images that are similar, but not identical.\\n3. Predictive learning: This involves training a model to predict the output of a transformation, rather than the transformation itself. For example, a model might be trained to predict the output of a rotation, rather than the rotation itself.\\n\\nSSL has several advantages over traditional supervised learning, including:\\n\\n1. Reduced need for labeled data: SSL can be used with unlabeled data, which can be easier to obtain than labeled data.\\n2. Improved robustness: SSL can learn representations that are invariant to certain transformations, which can make the model more robust to changes in the data.\\n3. Improved generalization: SSL can learn representations that are useful for a wide range of tasks, which can improve the model's ability to generalize to new data.\\n\\nHowever, SSL also has some disadvantages, including:\\n\\n1. Increased computational cost: SSL can be computationally expensive, particularly for large datasets.\\n2. Difficulty in choosing hyperparameters: SSL requires careful choice of hyperparameters, such as the number of negative examples and the learning rate.\\n3. Limited interpretability: SSL can be difficult to interpret, particularly for complex models.\\n\\nIn conclusion, self-supervised learning is a powerful paradigm in machine learning that can be used to learn representations that are useful for a wide range of tasks, without the need for large amounts of labeled data. While it has several advantages over traditional supervised learning, it also has some disadvantages that need to be carefully considered.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fdf52511-106b-49e7-95f6-2fcee406f7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What are the recent advances in representation learning using self-supervised methods, and how do they compare with supervised approaches?', additional_kwargs={}, response_metadata={}, id='864f64cb-58dc-44a7-8cda-120a5590828e'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '8nm150kmt', 'function': {'arguments': '{\"query\":\"Recent advances in representation learning using self-supervised methods and comparison with supervised approaches.\"}', 'name': 'pmc_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 616, 'total_tokens': 648, 'completion_time': 0.055711948, 'completion_tokens_details': None, 'prompt_time': 0.034180196, 'prompt_tokens_details': None, 'queue_time': 0.049012634, 'total_time': 0.089892144}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019be65f-62c8-7bf2-9022-02537da89f35-0', tool_calls=[{'name': 'pmc_retriever', 'args': {'query': 'Recent advances in representation learning using self-supervised methods and comparison with supervised approaches.'}, 'id': '8nm150kmt', 'type': 'tool_call'}], usage_metadata={'input_tokens': 616, 'output_tokens': 32, 'total_tokens': 648}),\n",
       "  ToolMessage(content='Thus, to build effective models in various application areas different types of machine learning techniques can play a significant role according to their learning capabilities, depending on the nature of the data discussed earlier, and the target outcome. In Table 1, we summarize various types of machine learning techniques with examples. In the following, we provide a comprehensive view of machine learning algorithms that can be applied to enhance the intelligence and capabilities of a data-driven application.\\nTable 1.\\nVarious types of machine learning techniques with examples\\n\\n\\nLearning type\\nModel building\\nExamples\\n\\n\\n\\nSupervised\\nAlgorithms or models learn from labeled data (task-driven approach)\\nClassification, regression\\n\\n\\nUnsupervised\\nAlgorithms or models learn from unlabeled data (Data-Driven Approach)\\nClustering, associations, dimensionality reduction\\n\\n\\nSemi-supervised\\nModels are built using combined data (labeled + unlabeled)\\nClassification, clustering\\n\\nAn example of a convolutional neural network (CNN or ConvNet) including multiple convolution and pooling layersIn addition to these most common deep learning methods discussed above, several other deep learning approaches [96] exist in the area for various purposes. For instance, the self-organizing map (SOM) [58] uses unsupervised learning to represent the high-dimensional data by a 2D grid map, thus achieving dimensionality reduction. The autoencoder (AE) [15] is another learning technique that is widely used for dimensionality reduction as well and feature extraction in unsupervised learning tasks. Restricted Boltzmann machines (RBM) [46] can be used for dimensionality reduction, classification, regression, collaborative filtering, feature learning, and topic modeling. A deep belief network (DBN) is typically composed of simple, unsupervised networks such as restricted Boltzmann machines (RBMs) or autoencoders, and a backpropagation neural network (BPNN) [123]. A generative\\n\\nSemi-supervised: Semi-supervised learning can be defined as a hybridization of the above-mentioned supervised and unsupervised methods, as it operates on both labeled and unlabeled data [41, 105]. Thus, it falls between learning “without supervision” and learning “with supervision”. In the real world, labeled data could be rare in several contexts, and unlabeled data are numerous, where semi-supervised learning is useful [75]. The ultimate goal of a semi-supervised learning model is to provide a better outcome for prediction than that produced using the labeled data alone from the model. Some application areas where semi-supervised learning is used include machine translation, fraud detection, labeling data and text classification.\\n\\nMany classification algorithms have been proposed in the machine learning and data science literature [41, 125]. In the following, we summarize the most common and popular methods that are used widely in various application areas.', name='pmc_retriever', id='75f5d7e5-7f80-454e-91f5-de6632c1eeb7', tool_call_id='8nm150kmt'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'hh57r6sam', 'function': {'arguments': '{\"query\":\"Comparison of self-supervised and supervised representation learning methods.\"}', 'name': 'pmc_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 1229, 'total_tokens': 1256, 'completion_time': 0.034491168, 'completion_tokens_details': None, 'prompt_time': 0.068542067, 'prompt_tokens_details': None, 'queue_time': 0.050578553, 'total_time': 0.103033235}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019be65f-63e1-71c0-9928-892596174400-0', tool_calls=[{'name': 'pmc_retriever', 'args': {'query': 'Comparison of self-supervised and supervised representation learning methods.'}, 'id': 'hh57r6sam', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1229, 'output_tokens': 27, 'total_tokens': 1256}),\n",
       "  ToolMessage(content='Thus, to build effective models in various application areas different types of machine learning techniques can play a significant role according to their learning capabilities, depending on the nature of the data discussed earlier, and the target outcome. In Table 1, we summarize various types of machine learning techniques with examples. In the following, we provide a comprehensive view of machine learning algorithms that can be applied to enhance the intelligence and capabilities of a data-driven application.\\nTable 1.\\nVarious types of machine learning techniques with examples\\n\\n\\nLearning type\\nModel building\\nExamples\\n\\n\\n\\nSupervised\\nAlgorithms or models learn from labeled data (task-driven approach)\\nClassification, regression\\n\\n\\nUnsupervised\\nAlgorithms or models learn from unlabeled data (Data-Driven Approach)\\nClustering, associations, dimensionality reduction\\n\\n\\nSemi-supervised\\nModels are built using combined data (labeled + unlabeled)\\nClassification, clustering\\n\\nSemi-supervised: Semi-supervised learning can be defined as a hybridization of the above-mentioned supervised and unsupervised methods, as it operates on both labeled and unlabeled data [41, 105]. Thus, it falls between learning “without supervision” and learning “with supervision”. In the real world, labeled data could be rare in several contexts, and unlabeled data are numerous, where semi-supervised learning is useful [75]. The ultimate goal of a semi-supervised learning model is to provide a better outcome for prediction than that produced using the labeled data alone from the model. Some application areas where semi-supervised learning is used include machine translation, fraud detection, labeling data and text classification.\\n\\nOpen in a new tab\\nVarious types of machine learning techniques\\nSupervised: Supervised learning is typically the task of machine learning to learn a function that maps an input to an output based on sample input-output pairs [41]. It uses labeled training data and a collection of training examples to infer a function. Supervised learning is carried out when certain goals are identified to be accomplished from a certain set of inputs [105], i.e., a task-driven approach. The most common supervised tasks are “classification” that separates the data, and “regression” that fits the data. For instance, predicting the class label or sentiment of a piece of text, like a tweet or a product review, i.e., text classification, is an example of supervised learning.\\n\\nAn example of a convolutional neural network (CNN or ConvNet) including multiple convolution and pooling layersIn addition to these most common deep learning methods discussed above, several other deep learning approaches [96] exist in the area for various purposes. For instance, the self-organizing map (SOM) [58] uses unsupervised learning to represent the high-dimensional data by a 2D grid map, thus achieving dimensionality reduction. The autoencoder (AE) [15] is another learning technique that is widely used for dimensionality reduction as well and feature extraction in unsupervised learning tasks. Restricted Boltzmann machines (RBM) [46] can be used for dimensionality reduction, classification, regression, collaborative filtering, feature learning, and topic modeling. A deep belief network (DBN) is typically composed of simple, unsupervised networks such as restricted Boltzmann machines (RBMs) or autoencoders, and a backpropagation neural network (BPNN) [123]. A generative', name='pmc_retriever', id='a1927bf7-81bf-4ddd-9ae0-c39799341fec', tool_call_id='hh57r6sam'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'g4c0jw5vj', 'function': {'arguments': '{\"query\":\"Comparison of self-supervised and supervised representation learning methods.\"}', 'name': 'wikipedia'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 1948, 'total_tokens': 1971, 'completion_time': 0.030377797, 'completion_tokens_details': None, 'prompt_time': 0.114974635, 'prompt_tokens_details': None, 'queue_time': 0.049869715, 'total_time': 0.145352432}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019be65f-64de-7f93-b0da-f8b6ea6a6c86-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'Comparison of self-supervised and supervised representation learning methods.'}, 'id': 'g4c0jw5vj', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1948, 'output_tokens': 23, 'total_tokens': 1971}),\n",
       "  ToolMessage(content='Page: Self-supervised learning\\nSummary: Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rathe', name='wikipedia', id='ac6a6c8d-e0ea-41f4-97eb-c62169df7acf', tool_call_id='g4c0jw5vj'),\n",
       "  AIMessage(content=\"Self-supervised learning (SSL) is a paradigm in machine learning where a model is trained on a task using the data itself to generate supervisory signals, rather than relying on human-provided labels. This approach has gained popularity in recent years due to its ability to learn representations that are useful for a wide range of tasks, without the need for large amounts of labeled data.\\n\\nOne of the key advantages of SSL is that it can be used to learn representations that are invariant to certain transformations, such as rotations or translations. This is because the model is trained to predict the output of a transformation, rather than the transformation itself. For example, a model trained on images of objects with different rotations would learn to represent the objects in a way that is invariant to rotation.\\n\\nSSL can be used in a variety of applications, including computer vision, natural language processing, and speech recognition. It has been shown to be particularly effective in tasks where the data is high-dimensional and the number of labeled examples is limited.\\n\\nThere are several types of SSL, including:\\n\\n1. Autoencoders: These are neural networks that learn to compress and reconstruct data. They can be used for SSL by training the model to reconstruct the input data, rather than predicting a label.\\n2. Contrastive learning: This involves training a model to distinguish between positive and negative examples. For example, a model might be trained to distinguish between two images that are similar, but not identical.\\n3. Predictive learning: This involves training a model to predict the output of a transformation, rather than the transformation itself. For example, a model might be trained to predict the output of a rotation, rather than the rotation itself.\\n\\nSSL has several advantages over traditional supervised learning, including:\\n\\n1. Reduced need for labeled data: SSL can be used with unlabeled data, which can be easier to obtain than labeled data.\\n2. Improved robustness: SSL can learn representations that are invariant to certain transformations, which can make the model more robust to changes in the data.\\n3. Improved generalization: SSL can learn representations that are useful for a wide range of tasks, which can improve the model's ability to generalize to new data.\\n\\nHowever, SSL also has some disadvantages, including:\\n\\n1. Increased computational cost: SSL can be computationally expensive, particularly for large datasets.\\n2. Difficulty in choosing hyperparameters: SSL requires careful choice of hyperparameters, such as the number of negative examples and the learning rate.\\n3. Limited interpretability: SSL can be difficult to interpret, particularly for complex models.\\n\\nIn conclusion, self-supervised learning is a powerful paradigm in machine learning that can be used to learn representations that are useful for a wide range of tasks, without the need for large amounts of labeled data. While it has several advantages over traditional supervised learning, it also has some disadvantages that need to be carefully considered.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 575, 'prompt_tokens': 2023, 'total_tokens': 2598, 'completion_time': 0.801615251, 'completion_tokens_details': None, 'prompt_time': 0.113298798, 'prompt_tokens_details': None, 'queue_time': 0.063369862, 'total_time': 0.914914049}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019be65f-6f5b-7a93-b293-2d05f1677953-0', usage_metadata={'input_tokens': 2023, 'output_tokens': 575, 'total_tokens': 2598})]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dfd723-67cb-43f7-9cb4-f56ac96015f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
